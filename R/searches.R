# SEARCH FUNCTIONS

# Functions to generate URLs and call APIs

# Generics

#' Pick out nodes from xml and turn it into a tibble
#' 
#' @param xmlnode an xml node containing nodes
#' @param nodenames the names of the nodes you wish to extract (string with nodes separated by commas)
#' @import dplyr
#' @import tidyr
#' @importFrom xml2 xml_text xml_name
#' @return A tibble with node names as col names and node text as values
#'  

xml2tib <- function(xmlnode, nodenames) {
  
  values <- xmlnode  %>%
    xml_nodes(nodenames) %>% 
    xml2::xml_text() %>% 
    as_tibble()
  
  fields <- xmlnode %>% 
    xml_nodes(nodenames) %>% 
    xml_name() %>% 
    as_tibble() %>% 
    rename(field = value)
  
  bind_cols(values, fields) %>% 
    group_by(field) %>% 
    mutate(value = paste0(value, collapse = " ; ")) %>% 
    unique() %>% 
    ungroup() 
  
}
  
# Pubmed

#' Generates URL for pubmed API
#'
#' @param searchterm text of the query
#' @param datefrom from date appeared online (default = 1 year ago today), format YYYY/MM/DD
#' @param dateto to date appeared online (default = today), format YYYY/MM/DD
#' @import dplyr
#' @import stringr
#' @return string, a URL to search pubmed for a particular term between two given dates
#' 
gen_url_pm <- function(searchterm, 
                         datefrom=Sys.Date()-365, 
                         dateto=Sys.Date()) {
  
  baseurl <- "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"
  
  term <- searchterm %>% 
    str_replace_all(., "(\")([^\" ]+)( )([^\" ]+)(\")", "%22\\2+\\4%22") %>% 
    str_replace_all(., "(\\))", " \\1") %>%
    paste0(., " ") %>% 
    str_replace_all(., " ", "[tiab] ") %>% 
    str_replace_all(., fixed("AND[tiab]"), "AND") %>% 
    str_replace_all(., fixed("OR[tiab]"), "OR") %>% 
    str_replace_all(., fixed("NOT[tiab]"), "NOT") %>% 
    str_replace_all(., fixed(")[tiab]"), ")") %>% 
    str_squish() %>% 
    str_replace_all(., " ", "+")
  
  datefrom <- as.character(datefrom, "%Y/%m/%d")
  dateto <- as.character(dateto, "%Y/%m/%d")
  
  searchurl <- paste0(baseurl,
                      "db=pubmed&term=",term,
                      "&datetype=pdat&mindate=",datefrom,"&maxdate=",dateto,
                      "&usehistory=y")
  return(searchurl)
}

#' Pubmed search
#' 
#' @param searchurl search URL generated by gen_url_pm()
#' @import httr
#' @importFrom rvest xml_nodes
#' @importFrom xml2 xml_text
#' @return A list with the total hits and history parameters for returning article info
#' 
search_pm <- function(searchurl) {
  
  keyenv <- GET(searchurl) %>%
    content() %>%
    xml_nodes("Count, QueryKey, WebEnv") %>%
    xml2::xml_text()
  
  return(list(count = keyenv[1], querykey = keyenv[2], webenv = keyenv[3]))
  
}

#' Pubmed fetch one page (up to 500 refs) from search
#' 
#' @param pagenumber number of page to retrieve
#' @param historyinfo web environment info returned from search_pm()
#' @import dplyr
#' @import purrr
#' @import tidyr
#' @import httr
#' @importFrom rvest xml_nodes
#' @importFrom xml2 xml_text xml_name
#' @return tibble with info fields on up to 500 articles
#' 
fetch_pm <- function(pagenumber, historyinfo) {
  
  url <- paste0("https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=",
                historyinfo$querykey,"&WebEnv=",historyinfo$webenv,"&retmax=500&retstart=",pagenumber,"&retmode=xml")
  
  articlexml <- GET(url) %>% content() %>% xml_nodes("PubmedArticle")

  articleinfo <- map_df(articlexml, function(x) xml2tib(x, "ArticleTitle, 
                              Abstract, 
                              ArticleId[IdType=\"doi\"], 
                              Journal Title, 
                              PublicationStatus, 
                              PublicationType, 
                              PubDate Year,
                              PubDate Month,
                              PubDate Day,
                              Author LastName, 
                              Language")  %>% spread(field, value))
  
  return(articleinfo)
}

#' Pubmed all steps
#' 
#' @param searchterm text of the query
#' @param datefrom from date appeared online (default = 1 year ago today), format YYYY/MM/DD
#' @param dateto to date appeared online (default = today), format YYYY/MM/DD
#' @return a tibble of results
#' 
get_pm <- function(searchterm,
                   datefrom=Sys.Date()-365, 
                   dateto=Sys.Date()) {
  
  url <- gen_url_pm(searchterm, datefrom, dateto)

  search <- search_pm(url)
  
  if(as.numeric(search$count) > 0 ) {

    pages <- seq(0, search$count, 500)
  
    results <- map_df(pages, fetch_pm, search) %>%
      filter(!is.na(Year)) %>%
      mutate_at(vars(Day, Month), ~if_else(is.na(.), "01", .)) %>%
      mutate(Month = case_when(
        Month == "Jan" ~ "01",
        Month == "Feb" ~ "01",
        Month == "Mar" ~ "01",
        Month == "Apr" ~ "04",
        Month == "May" ~ "01",
        Month == "Jun" ~ "01",
        Month == "Jul" ~ "01",
        Month == "Aug" ~ "01",
        Month == "Sep" ~ "01",
        Month == "Oct" ~ "01",
        Month == "Nov" ~ "01",
        Month == "Dec" ~ "01",
        TRUE ~ Month
    )) %>%
      mutate(pdate = paste0(Year,"-",Month,"-",Day)) %>%
      mutate(type = "") %>%
      mutate(type = if_else(grepl("^Journal Article ;|; Journal Article ;|; Journal Article$|^Journal Article$", PublicationType), "journal article", type)) %>%
      mutate(type = if_else(grepl("^Review ;|; Review ;| ; Review$|^Review$", PublicationType), "review", type)) %>%
      mutate(type = if_else(type == "", "other", type)) %>%
      mutate(url = paste0("https://dx.doi.org/",ArticleId)) %>% 
      select(doi = ArticleId,
             title = ArticleTitle,
             abstract = Abstract,
             author = LastName,
             `publication date` = pdate,
             `publication type` = type,
             journal = Title,
             lang = Language, 
             url) %>% 
      mutate(source = "Pubmed") %>% 
      filter(!is.na(doi)) %>% 
      group_by(doi) %>% 
      mutate(id = row_number()) %>% 
      ungroup() %>% 
      filter(id == 1) %>% 
      select(-id)
    
  } else {
    
    results <- tibble(doi = character(0))
    
  }
  
  return(results)
  
}

#' Make URL for Springer API
#' 
#' @param searchterm text of query
#' @param datefrom earliest date added
#' @param dateto latest date added
#' @param apikey Springer API key
#' @import stringr
#' 
gen_url_springer <- function(searchterm,
                             datefrom = as.character(Sys.Date() - 365),
                             dateto = as.character(Sys.Date()),
                             apikey = Sys.getenv("SPRINGER_API")) {
  
  baseurl <- "http://api.springernature.com/meta/v2/json?"

  terms <- searchterm %>% 
    str_split(., " AND | OR | NOT ") %>% 
    .[[1]] %>% 
    str_remove_all(., "[\\(\\)]") %>% 
    str_replace_all(., "\"", "%22") %>%
    map_chr(., str_squish) %>% 
    str_c(., collapse = "|") %>% 
    paste0("(",.,")")
  
  term <- searchterm %>%
    str_replace_all(., "\"", "%22") %>%
    str_replace_all(., terms, "title:\\1") %>% 
    str_replace_all(., " ", "+") 
  
  searchurl <- paste0(baseurl,
                      "q=",term,"+onlinedatefrom:",datefrom,"+onlinedateto:",dateto,
                      "&api_key=",apikey)
  return(searchurl)
}

#' Springer fetch 1 page (up to 100 per page)
#' 
#' @param searchurl URL with the search query
#' @param page page to fetch
#' @import dplyr
#' @importFrom jsonlite fromJSON
#' @import purrr
#' @import httr
#' 
get_results_springer <- function(page, searchurl) {

    pagurl <- paste0(searchurl,"&p=100&s=",page)
    
    fullspring <- GET(pagurl) %>% 
      content(., "text") %>% 
      fromJSON() %>% 
      .$records %>% 
      as_tibble() %>% 
      select(url, title, creators, publicationName, doi, publicationDate, publicationType,
             genre, abstract) %>% 
      mutate_at(vars(url, creators, genre), ~as.list(.))
    
    return(fullspring)
  }
  
  
#' Springer all steps
#'  
#' @param searchterm text of query
#' @param datefrom earliest date added
#' @param dateto latest date added
#' @param apikey Springer API key
#' @return a tibble with 11 columns    

get_springer <- function(searchterm,
                         datefrom = as.character(Sys.Date() - 365),
                         dateto = as.character(Sys.Date()),
                         apikey = Sys.getenv("SPRINGER_API"))  {
  
  searchurl <- gen_url_springer(searchterm)

  total <- GET(searchurl) %>% 
    content(., "text") %>% 
    fromJSON() %>% 
    .$result %>% .$total 
  
  if(as.numeric(total) > 0) {
  
    # paginate
    
    pages <- seq(1, total, 100)
    
    # map across pages
    
    result <- map_df(pages, get_results_springer, searchurl = searchurl)
  
    # clean
      
      urls <- result %>% 
        group_by(doi) %>% 
        unnest(cols = "url") %>% 
        filter(format == "html") %>% 
        ungroup() %>% 
        select(doi, url = value) 
  
      authors <- result %>% 
        group_by(doi) %>%
        unnest(cols = "creators") %>%
        mutate(createlist = paste0(creator, collapse = " ; ")) %>%
        select(-creator) %>%
        ungroup() %>%
        unique() %>%
        select(doi, author = createlist)
  
      types <- result %>% 
        group_by(doi) %>%
        select(doi, publicationType, genre) %>%
        unnest(cols = "genre") %>%
        mutate(genre = paste0(genre, collapse = " ; ")) %>%
        unique() %>%
        mutate(type = "") %>%
        mutate(type = if_else(publicationType == "Journal" &
                                grepl("original( )?paper|^article$|original( )?article|research( )?article|^research$|research( )?paper|original( )?contribution",
                                      genre, ignore.case = T),
                              "journal article", type)) %>%
        mutate(type = if_else(publicationType == "Journal" &
                                grepl("review( )?paper|review( )?article|invited( )review|^review$",
                                      genre, ignore.case = T),
                              "review", type)) %>%
        mutate(type = if_else(type == "", "other", type)) %>%
        select(-genre) %>%
        unique() %>%
        ungroup()
      
      result <- result %>%
        left_join(urls, by = "doi") %>%
         left_join(authors, by = "doi") %>%
         left_join(types, by = "doi") %>%
         select(doi,
                title,
                abstract,
                author,
                `publication date` = publicationDate,
                `publication type` = type,
                journal = publicationName,
                url = url.y) %>% 
        mutate(source = "Springer",
               lang = NA) %>% 
        filter(!is.na(doi)) %>% 
        group_by(doi) %>% 
        mutate(id = row_number()) %>% 
        ungroup() %>% 
        filter(id == 1) %>% 
        select(-id)

  
   } else {
  
     result <- tibble(doi = character(0))
  
   }
  
   return(result)
  
}

# SCOPUS

#' Scopus generate URL
#' 
#' @param searchterm string with search term
#' @param dateto search articles published until (default today)
#' @param datefrom search articles published from (default one year ago)
#' @param cursor code for cursor (defaults to asterisk for first page)
#' @import dplyr
#' @import stringr
#' @return a string with the URL to hit
#' 
gen_url_scopus <- function(searchterm, dateto = Sys.Date(), datefrom = Sys.Date()-365, cursor = "*") {
  
  query <- searchterm %>% 
    str_replace_all(., "\"", "%22") %>% 
    str_replace_all(., " ", "+")
  
  # dates need to be replaced with years as this is as granular as it goes!
  
  dates <- paste0("date=",substr(as.character(datefrom), 1, 4),"-",substr(as.character(dateto), 1, 4))
  
  url <- paste0("https://api.elsevier.com/content/search/scopus?query=title-abs-key(",query,")&",dates,"&view=COMPLETE&count=25&cursor=",cursor)
  
  return(url)
  
}

#' Scopus get page of results
#' 
#' @param url URL to hit
#' @import httr
#' @import dplyr
#' @importFrom magrittr extract
#' @import purrr
#' @return a list with a tibble of results, a URL for the next page, and the total number of pages
#' 
get_scopus_result <- function(url) {
  
  hit <- GET(url,
             add_headers(.headers = c(`X-ELS-APIKey` = Sys.getenv("ELSEVIER_API_KEY"),
                                      `X-ELS-Insttoken` = Sys.getenv("ELSEVIER_INST_TOKEN")))) %>% 
    content() %>% 
    .$`search-results`
  
  rcount <- hit %>% .$`opensearch:totalResults` %>% as.numeric()
  
  if(rcount == 0) {
    
    result <- tibble(doi = character(0))
    nextpage <- NA
    
  } else if(rcount > 0) {
    
    nextpage <- hit %>% .$link %>% map(., data.frame) %>% bind_rows() %>% filter(X.ref == "next") %>% pull(X.href) %>% as.character()
    
    if(rcount <= 25) {
      nextpage <- NA
    } else {
      nextpage <- nextpage
    }
    
    meta <- hit %>% .$entry %>% map_df(., function(x) extract(x, c("dc:title", 
                                                                   "prism:publicationName",
                                                                   "prism:coverDate",
                                                                   "prism:doi",
                                                                   "dc:description",
                                                                   "subtypeDescription",
                                                                   "prism:aggregationType",
                                                                   "prism:url")) %>% 
                                         flatten_df())
    
    authors <- hit %>% 
      .$entry %>% 
      map(., function(x) map(x$author, "authname") %>% paste(., collapse = " ; ")) %>% 
      map(., function(x) ifelse(is.null(x), NA, x)) %>% 
      flatten_chr()
    
    result <- meta %>% 
      mutate(author = authors) %>% 
      mutate(pubtype = paste(`prism:aggregationType`, `subtypeDescription`)) %>% 
      mutate(pubtype = case_when(pubtype == "Journal Article" ~ "journal article",
                                 pubtype == "Journal Review" ~ "review", 
                                 TRUE ~ "other")) %>% 
      select(doi = `prism:doi`, 
             title = `dc:title`,
             abstract = `dc:description`,
             author,
             `publication date` = `prism:coverDate`,
             `publication type` = pubtype,
             journal = `prism:publicationName`,
             url = `prism:url`) %>% 
      mutate(source = "Scopus",
             lang = NA) %>% 
      filter(!is.na(doi)) %>% 
      group_by(doi) %>% 
      mutate(id = row_number()) %>% 
      ungroup() %>% 
      filter(id == 1) %>% 
      select(-id)
    
  }
  
  return(list(result, nextpage, rcount))
  
}

#' Scopus all steps
#' 
#' @param searchterm string with search term
#' @param dateto search articles published until (default today)
#' @param datefrom search articles published from (default one year ago)
#' @param cursor code for cursor (defaults to asterisk for first page)
#' @import dplyr
#' @import purrr
#' @return a tibble with all results
#' 
get_scopus <- function(searchterm, dateto = Sys.Date(), datefrom = Sys.Date()-365, cursor = "*") {
  
  df <- get_scopus_result(gen_url_scopus(searchterm))
  first <- df
  
  if(df[[3]] == 0) {
    
    result <- tibble(doi = character(0))
    
  } else if(df[[3]] > 0 & df[[3]] <= 25) {
    
    result <- df[[1]]
    
  } else {
    
    pages <- ceiling(df[[3]] / 25) - 1
    
    restof <- vector("list", pages)
    
    for(i in 1:pages) {
      df <- get_scopus_result(df[[2]])
      restof[[i]] <- df[[1]]
    }
    
    result <- bind_rows(first[[1]], restof)
    
  }
  
  return(result)
  
}

#' Scopus missing abstracts
#' 
#' Fetches forbidden scopus abstracts from sciencedirect
#' 
#' @param doi string of DOI of article abstract to retrieve
#' @import dplyr
#' @import stringr
#' @return a tibble with all results
#'
getab <- function(doi) {
  
  abstract <- GET(paste0("https://api.elsevier.com/content/article/doi/",doi),
      add_headers(.headers = c(`X-ELS-APIKey` = Sys.getenv("ELSEVIER_API_KEY"),
                               `X-ELS-Insttoken` = Sys.getenv("ELSEVIER_INST_TOKEN")))) %>% 
    content() %>% 
    .$`full-text-retrieval-response` %>% 
    .$coredata %>% 
    .$`dc:description` %>% 
    str_squish()
  
  tibble(doi = doi, altab = abstract)
}

